# -*- coding: utf-8 -*-

import numpy as np
import cv2
import tensorflow as tf
import os
import tensorflow.contrib.tensorrt as trt
import sys
import urllib
import time
import paho.mqtt.client as mqtt


#Create mqtt publishing client
client = mqtt.Client()
client.connect("mosquitto", 1883, keepalive=65500)

cap = cv2.VideoCapture(1)

FROZEN_GRAPH_NAME = 'data/frozen_inference_graph_face.pb'

output_dir=''
frozen_graph = tf.GraphDef()
with open(os.path.join(output_dir, FROZEN_GRAPH_NAME), 'rb') as f:
  frozen_graph.ParseFromString(f.read())

INPUT_NAME='image_tensor'
BOXES_NAME='detection_boxes'
CLASSES_NAME='detection_classes'
SCORES_NAME='detection_scores'
MASKS_NAME='detection_masks'
NUM_DETECTIONS_NAME='num_detections'

input_names = [INPUT_NAME]
output_names = [BOXES_NAME, CLASSES_NAME, SCORES_NAME, NUM_DETECTIONS_NAME]

trt_graph = trt.create_inference_graph(
    input_graph_def=frozen_graph,
    outputs=output_names,
    max_batch_size=1,
    max_workspace_size_bytes=1 << 25,
    precision_mode='FP16',
    minimum_segment_size=50
)

tf_config = tf.ConfigProto()
tf_config.gpu_options.allow_growth = True

tf_sess = tf.Session(config=tf_config)

# use this if you want to try on the optimized TensorRT graph
# Note that this will take a while
# tf.import_graph_def(trt_graph, name='')

# use this if you want to try directly on the frozen TF graph
# this is much faster
tf.import_graph_def(frozen_graph, name='')

tf_input = tf_sess.graph.get_tensor_by_name(input_names[0] + ':0')
tf_scores = tf_sess.graph.get_tensor_by_name('detection_scores:0')
tf_boxes = tf_sess.graph.get_tensor_by_name('detection_boxes:0')
tf_classes = tf_sess.graph.get_tensor_by_name('detection_classes:0')
tf_num_detections = tf_sess.graph.get_tensor_by_name('num_detections:0')

while(True):
	# Find OpenCV version
	(major_ver, minor_ver, subminor_ver) = (cv2.__version__).split('.')

	# With webcam get(CV_CAP_PROP_FPS) does not work.
	# Let's see for ourselves.
	if int(major_ver)  < 3 :
		fps = cap.get(cv2.cv.CV_CAP_PROP_FPS)
		print("Frames per second using video.get(cv2.cv.CV_CAP_PROP_FPS): {0}".format(fps))
	else :
		fps = cap.get(cv2.CAP_PROP_FPS)
		print("Frames per second using video.get(cv2.CAP_PROP_FPS) : {0}".format(fps))

	ret, frame = cap.read()
	gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

	scores, boxes, classes, num_detections = tf_sess.run([tf_scores, tf_boxes, tf_classes, tf_num_detections], feed_dict={tf_input: frame[None, ...]})

	boxes = boxes[0] # index by 0 to remove batch dimension
	scores = scores[0]
	classes = classes[0]
	num_detections = num_detections[0]

	DETECTION_THRESHOLD = 0.5

    # plot boxes exceeding score threshold
	for i in range(int(num_detections)):
		if scores[i] < DETECTION_THRESHOLD:
			continue

        # scale box to image coordinates
	box = boxes[i] * np.array([frame.shape[0], frame.shape[1], frame.shape[0], frame.shape[1]])

	crop_img = frame[int(box[0]):int(box[2]),int(box[1]):int(box[3])] 
	cv2.imshow("cropped", crop_img)

	# save clipped image into memory as a byte arrary
	img_str = cv2.imencode('.png',crop_img)[1]
	msg = img_str.tobytes()

        #publish to broker using QoS = 0 protocol which does not require confirmation to increase processing speed.
	client.publish(topic = "image", payload = msg, qos=0)
	client.loop(5)

    # display live video feed for debugging purposes 
	cv2.imshow('frame',gray)
	if cv2.waitKey(1) & 0xFF == ord('q'):
        	break

# When everything done, release the capture
cap.release()
cv2.destroyAllWindows()
